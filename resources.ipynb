# print(torch.__version__)
# print(torch.version.cuda)
# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.9.1+cu111.html
# !pip install torch-sparse -f https://data.pyg.org/whl/torch-1.9.1+cu111.html
# !pip install torch-geometric

from torch_geometric.datasets import Planetoid
import torch
import torch.nn.functional as F
from torch.nn import Linear
from torch.utils.data import random_split
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
import pickle
import time

class Graph(object):
    def __init__(self, node_list, edge_list, dict_idx2number, dict_idx2name, dict_idx2coord):
        super(Graph, self).__init__()
        self.nodes = node_list
        self.edges = edge_list
        self.idx2number = dict_idx2number
        self.idx2name = dict_idx2name
        self.idx2coord = dict_idx2coord
    def to(self, dev):
        self.nodes = self.nodes.to(dev)
        self.edges = self.edges.to(dev)

def construct_graph_from_dataset(dataset):
    g = Graph(dataset.nodes, dataset.edges, dataset.idx2number, dataset.idx2name, dataset.idx2coord)
    return g

def plot_dataset(graph):
    fig, ax = plt.subplots(figsize=(25,25))

    edge_list = graph.edges.numpy()
    edges = [(x, y) for x, y in zip(edge_list[0, :], edge_list[1, :])]

    G = nx.Graph()
    G.add_nodes_from(list(range(np.max(edge_list))))
    G.add_edges_from(edges)
    plt.subplot(111)
    options = {
                'node_size': 100,
                'width': 1,
    }
    nx.draw(G, labels=graph.idx2name, cmap=plt.cm.tab10, font_weight='bold', pos=graph.idx2coord, **options)
    ax.set_aspect('equal', 'box')
    plt.show()

def create_masks(data):
    len_data = data.shape[0]
    
    full_dataset = torch.arange(0, len_data)
    train_size = int(0.8 * len(full_dataset))
    valid_size = int(0.1 * len(full_dataset))
    test_size = len(full_dataset) - train_size - valid_size
    train_subset, valid_subset, test_subset = random_split(full_dataset, [train_size, valid_size, test_size])

    train_mask = torch.zeros(len_data).bool()
    valid_mask = torch.zeros(len_data).bool()
    test_mask = torch.zeros(len_data).bool()

    for i in range(train_subset.__len__()):
        train_mask[train_subset.__getitem__(i)] = True
    for i in range(valid_subset.__len__()):
        valid_mask[valid_subset.__getitem__(i)] = True
    for i in range(test_subset.__len__()):
        test_mask[test_subset.__getitem__(i)] = True

    return train_mask, valid_mask, test_mask
    
class GCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels, node_dim=-2):
        super(GCNConv, self).__init__(aggr='add')  # "Add" aggregation
        self.lin = torch.nn.Linear(in_channels, out_channels)
        self.node_dim = node_dim

    def forward(self, x, edge_index):
        # Step 1: Add self-loops
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(1))

        # Step 2: Multiply with weights
        x = self.lin(x)

        # Step 3: Calculate the normalization
        row, col = edge_index
        deg = degree(row, x.size(1), dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # Step 4: Propagate the embeddings to the next layer
        return self.propagate(edge_index, size=(x.size(1), x.size(1)), x=x,
                              norm=norm, node_dim=self.node_dim)

    def message(self, x_j, norm):
        # Normalize node features.
        return norm.view(-1, 1) * x_j


class Net(torch.nn.Module):
    def __init__(self, x, y, glob, num_out_feat):
        super(Net, self).__init__()
#         self.l_glob_out_features = 8
        
        self.num_node_features = x.shape[2]
        self.gcn2_features = 128
#         self.gcn3_features = 128
        self.gcn_out_features = y.shape[2]
        self.l_out_in_features = y.shape[2] + glob.shape[1] # self.l_glob_out_features # no. ODs per station + no. calendar features
        
#         self.l_glob = Linear(in_features=glob.shape[1], out_features=8)

        self.conv1 = GCNConv(self.num_node_features, self.gcn2_features, node_dim=1)
        self.conv2 = GCNConv(self.gcn2_features, self.gcn_out_features, node_dim=1)
#         self.conv3 = GCNConv(self.gcn3_features, self.gcn_out_features, node_dim=1)
        self.l_out = Linear(in_features=self.l_out_in_features, out_features=num_out_feat)

    def forward(self, x, glob, edge_list):
#         glob = self.l_glob(glob)
        
        x = self.conv1(x, edge_list)
        x = F.relu(x)
#         x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_list)
        x = F.relu(x)
#         x = self.conv3(x, edge_list)
#         x = F.relu(x)

        x = torch.cat((x, glob), dim=2)

        return F.relu(self.l_out(x))

class Net_FC(torch.nn.Module):
    def __init__(self, x, y, glob, num_out_feat):
        super(Net_FC, self).__init__()
        self.num_node_features = x.shape[2]
        self.fcn2_features = 128
        self.fcn3_features = 128
        self.fcn_out_features = y.shape[2]
        self.l_out_in_features = self.fcn_out_features + glob.shape[1] # no. ODs per station + no. calendar features

        self.l_1 = Linear(in_features=self.num_node_features, out_features=self.fcn2_features)
        self.l_2 = Linear(in_features=self.fcn2_features, out_features=self.fcn3_features)
        self.l_3 = Linear(in_features=self.fcn3_features, out_features=self.fcn_out_features)

        self.l_out = Linear(in_features=self.l_out_in_features, out_features=num_out_feat)
    
        self.num_nodes = x.shape[1]
        self.batchnorm = torch.nn.BatchNorm1d(self.num_nodes)

    def forward(self, x, glob, edge_list):
        
        x = self.l_1(x)
        x = F.relu(x)
#         x = self.batchnorm(x)

#         x = F.dropout(x, training=self.training)

        x = self.l_2(x)
        x = F.relu(x)
#         x = self.batchnorm(x)

        x = self.l_3(x)
        x = F.relu(x)

        x = torch.cat((x, glob), dim=2)

        return F.relu(self.l_out(x))

def train(x, y, glob, train_mask, valid_mask, test_mask, edge_index, net, optimizer, criterion, batch_size, num_epochs, plot=False):
#     train_mask = dataset.train_mask
#     valid_mask = dataset.valid_mask
#     test_mask = dataset.test_mask

    # Extract node feature values for training, validation and testing
    node_x_train = x[train_mask].float()
    node_x_valid = x[valid_mask].float()
    node_x_test = x[test_mask].float()

    # Extract global feature values for training, validation and testing
    global_x_train = glob[train_mask].float()
    global_x_valid = glob[valid_mask].float()
    global_x_test = glob[test_mask].float()

    # Extract response values for training, validation and testing
    y_train = y[train_mask].float()
    y_valid = y[valid_mask].float()
    y_test = y[test_mask].float()
    
    num_samples_train = node_x_train.shape[0]
    num_batches_train = num_samples_train // batch_size
    num_samples_valid = node_x_valid.shape[0]
    num_batches_valid = num_samples_valid // batch_size

    train_losses, valid_losses = list(), list()

    get_slice = lambda i, size: range(i * size, (i + 1) * size)
    
    for epoch in range(num_epochs):
        net.train()
        train_loss = 0.0
        for i in range(num_batches_train): #range(node_x_train.shape[0]):
            slce = get_slice(i, batch_size)
            # Extract data instance
            node_data = node_x_train[slce]

            # Global data for batch
            glob_data = torch.reshape(
                torch.repeat_interleave(global_x_train[slce], node_x_train.shape[1], dim=0), 
                (batch_size, node_x_train.shape[1], -1))

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            out = net(node_data, glob_data, edge_index)

            loss = criterion(out, y_train[slce])
            loss.backward()
            optimizer.step()

            train_loss += loss.data

        net.eval()
        valid_loss = 0.0
        for j in range(num_batches_valid): #range(node_x_valid.shape[0]):
            slce = get_slice(j, batch_size)
            # Extract data instance
            node_data = node_x_valid[slce]
            glob_data = torch.reshape(
                torch.repeat_interleave(global_x_valid[slce], node_x_valid.shape[1], dim=0), 
                (batch_size, node_x_valid.shape[1], -1))
            # forward + backward + optimize
            out = net(node_data, glob_data, edge_index)

            loss = criterion(out, y_valid[slce])

            valid_loss += loss.data
        
        train_losses.append(train_loss)
        valid_losses.append(valid_loss)

        if epoch + 1 == num_epochs:
            print('Epoch: {:03d}, Training Loss: {:.5f}, Validation loss: {:.5f}'.
                  format(epoch+1, train_loss/num_batches_train, valid_loss/num_batches_valid))
#         print("(training set size: ", num_batches_train, ")")
#         print("(validation set size: ", num_batches_valid, ")")

    if plot:
        plt.plot([tl/num_batches_train for tl in train_losses], label="Train loss")
        plt.plot([vl/num_batches_valid for vl in valid_losses], label="Validation loss")
        plt.xlabel("# Epoch")
        plt.ylabel("Loss")
        plt.legend(loc='upper right')
        plt.show()
    
    return train_losses, valid_losses, num_batches_train, num_batches_valid

# # No. samples for tiny dataset
# n_samples = 6400

# if False:
#     node_feat = node_feat[0:n_samples]
#     response = response[0:n_samples]
#     glob_feat = glob_feat[0:n_samples]

with open('graph_simple_2017.pkl', 'rb') as f:
    graph = pickle.load(f)

plot_dataset(graph)
